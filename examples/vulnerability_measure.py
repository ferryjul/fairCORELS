from sklearn.model_selection import train_test_split # version 0.24.1
from faircorels import load_from_csv, FairCorelsClassifier, ConfusionMatrix, Metric # version 1.0
import numpy as np
import csv

sensitive_attr_column = 0
unsensitive_attr_column = 1

datasets = ['adult_income', 'compas', 'german_credit', 'default_credit', 'marketing']
dataset = datasets[1]
X, y, features, prediction = load_from_csv("./data/%s_fullRules.csv" %dataset)#("./data/adult_full.csv") # Load the dataset

print("Sensitive attribute is ", features[sensitive_attr_column])
print("Unsensitive attribute is ", features[unsensitive_attr_column])

epsilon = 0.0 # Fairness constraint
fairnessMetric = 1 # 1 For Statistical Parity, 2 for Predictive Parity...etc
lambdaParam = 0.0#1e-3 # The regularization parameter penalizing rule lists length
N_ITER = 1*10**6 # The maximum number of nodes in the prefix tree
N_ITER = 1*10**8 # The maximum number of nodes in the prefix tree
time_limit = 10#0
nb_seeds = 100
method = 'FairCORELS'
policy = 'objective'
seed = 42

predict_modes = ['label_only', 'score', 'rule_id']
predict_proba_mode = predict_modes[2]

def compute_unfairness(sensVect, unSensVect, y, y_pred):
    cm = ConfusionMatrix(sensVect, unSensVect, y_pred, y)
    cm_minority, cm_majority = cm.get_matrix()
    fm = Metric(cm_minority, cm_majority)

    if fairnessMetric == 1:
        unf = fm.statistical_parity()
    elif fairnessMetric == 2:
        unf = fm.predictive_parity()
    elif fairnessMetric == 3:
        unf = fm.predictive_equality()
    elif fairnessMetric == 4:
        unf = fm.equal_opportunity()
    elif fairnessMetric == 5:
        unf = fm.equalized_odds()
    elif fairnessMetric == 6:
        unf = fm.conditional_use_accuracy_equality()
    else:
        unf = -1
    
    return unf

def target_model_fn():
    """The architecture of the target (victim) model.
    The attack is white-box, hence the attacker is assumed to know this architecture too."""
    # Create the FairCorelsClassifier object
    clf = FairCorelsClassifier(n_iter=N_ITER, 
                            c=lambdaParam, 
                            max_card=1, 
                            min_support = 0.01,
                            policy=policy, # exploration heuristic
                            bfs_mode=2, # exploration heuristic
                            mode=3, # epsilon-constrained mode
                            fairness=fairnessMetric, 
                            verbosity=[],
                            epsilon=epsilon, 
                            #maj_vect=unSensVect_train, 
                            #min_vect=sensVect_train
                            maj_pos=unsensitive_attr_column,
                            min_pos=sensitive_attr_column,
                            predict_proba_mode=predict_proba_mode,
                            time_limit = time_limit
                            )
    return clf

def compute_vulnerability_simpler(adversary, true_y_train, predicted_y_train, true_y_test, predicted_y_test, sens_vect_train, sens_vect_test, unsens_vect_train, unsens_vect_test, print_infos=False):
    predictions_train = np.asarray([predicted_y_train[i][1] for i in range(len(predicted_y_train))])
    predictions_test = np.asarray([predicted_y_test[i][1] for i in range(len(predicted_y_test))])

    advantage = 0
    if true_y_train.shape[0] != predicted_y_train.shape[0] or true_y_test.shape[0] != predicted_y_test.shape[0]:
        print("Shape errors")
        return -1
    # All existing true labels
    y_list = np.unique(np.union1d(true_y_train, true_y_test))
    # All existing confidence scores
    y_pred_list = np.unique(np.union1d(predicted_y_train, predicted_y_test))
    
    # Probas y
    probas_y = dict()
    for y in y_list:
        probas_y[y] = (np.sum(true_y_train == y)+np.sum(true_y_test == y))/(true_y_train.shape[0]+true_y_test.shape[0])
    if print_infos:
        print(probas_y)
    # Probas z
    probas_z = dict()
    probas_z['sens']=(np.sum(sens_vect_train == 1)+np.sum(sens_vect_test == 1))/(true_y_train.shape[0]+true_y_test.shape[0])
    probas_z['unsens']=(np.sum(unsens_vect_train == 1)+np.sum(unsens_vect_test == 1))/(true_y_train.shape[0]+true_y_test.shape[0])

    # Probas z intersect y = proba z * proba y sachant z
    probas_y_z = dict()
    probas_y_z['sens']=dict()
    probas_y_z['unsens']=dict()
    for y in y_list:
        nb_y_z_train = len(np.intersect1d(np.where(true_y_train == y), np.where(sens_vect_train == 1)))
        nb_y_z_test = len(np.intersect1d(np.where(true_y_test == y), np.where(sens_vect_test == 1)))
        probas_y_z['sens'][y]=(nb_y_z_train+nb_y_z_test)/(true_y_train.shape[0]+true_y_test.shape[0])
        nb_y_z_train = len(np.intersect1d(np.where(true_y_train == y), np.where(unsens_vect_train == 1)))
        nb_y_z_test = len(np.intersect1d(np.where(true_y_test == y), np.where(unsens_vect_test == 1)))
        probas_y_z['unsens'][y]=(nb_y_z_train+nb_y_z_test)/(true_y_train.shape[0]+true_y_test.shape[0])
    if print_infos:
        print(probas_y_z)

    if adversary == 'regular':
        for y in y_list:
            tau_y = 0
            for y_pred in y_pred_list:
                nb_y_pred_y_train = len(np.intersect1d(np.where(true_y_train == y), np.where(predictions_train == y_pred)))
                tot_y_train = np.sum(true_y_train == y)
                nb_y_pred_y_test = len(np.intersect1d(np.where(true_y_test == y), np.where(predictions_test == y_pred)))
                tot_y_test = np.sum(true_y_test == y)
                proba_y_pred_y_train=nb_y_pred_y_train/tot_y_train
                proba_y_pred_y_test=nb_y_pred_y_test/tot_y_test
                probas_diff=(proba_y_pred_y_train)-(proba_y_pred_y_test)
                tau_y+=abs(probas_diff)
                print("diff (y_pred=%d): %d/%d - %d/%d" %(y_pred, nb_y_pred_y_train, tot_y_train, nb_y_pred_y_test, tot_y_test))
            tau_y *= (1/2)
            if print_infos:
                print(">> Distributional Overfitting distance for class ", y , " : ", tau_y)
            advantage+=probas_y[y]*tau_y

    elif adversary == 'discriminating':
       for y in y_list:
            for z in ['sens', 'unsens']:
                tau_z_y = 0
                for y_pred in y_pred_list:
                    if z == 'sens':
                        nb_y_pred_y_train = len(np.intersect1d(np.intersect1d(np.where(true_y_train == y), np.where(predictions_train == y_pred)),np.where(sens_vect_train == 1)))
                        tot_y_train = len(np.intersect1d(np.where(true_y_train == y), np.where(sens_vect_train == 1)))
                        nb_y_pred_y_test = len(np.intersect1d(np.intersect1d(np.where(true_y_test == y), np.where(predictions_test == y_pred)),np.where(sens_vect_test == 1)))
                        tot_y_test = len(np.intersect1d(np.where(true_y_test == y), np.where(sens_vect_test == 1)))
                    elif z == 'unsens':
                        nb_y_pred_y_train = len(np.intersect1d(np.intersect1d(np.where(true_y_train == y), np.where(predictions_train == y_pred)),np.where(unsens_vect_train == 1)))
                        tot_y_train = len(np.intersect1d(np.where(true_y_train == y), np.where(unsens_vect_train == 1)))
                        nb_y_pred_y_test = len(np.intersect1d(np.intersect1d(np.where(true_y_test == y), np.where(predictions_test == y_pred)),np.where(unsens_vect_test == 1)))
                        tot_y_test = len(np.intersect1d(np.where(true_y_test == y), np.where(unsens_vect_test == 1)))
                    else:
                        print("Unexpected")
                    proba_y_pred_y_train=nb_y_pred_y_train/tot_y_train
                    proba_y_pred_y_test=nb_y_pred_y_test/tot_y_test
                    probas_diff=(proba_y_pred_y_train)-(proba_y_pred_y_test)
                    print("diff (y_pred=%d): %d/%d - %d/%d" %(y_pred, nb_y_pred_y_train, tot_y_train, nb_y_pred_y_test, tot_y_test))
                    tau_z_y+=abs(probas_diff)
                tau_z_y *= (1/2)
                if print_infos:
                    print(">> Distributional Overfitting distance for class ", y , " and group ", z, " : ", tau_z_y)
                advantage+=probas_y_z[z][y]*tau_z_y
    else:
        print("Unknown mode ", adversary)
    return 0.5 + 0.5*advantage # overall vulnerability value for regular adversary

def compute_vulnerability(adversary, true_y_train, predicted_y_train, true_y_test, predicted_y_test, sens_vect_train, sens_vect_test, unsens_vect_train, unsens_vect_test, print_infos=False):
    advantage = 0
    if true_y_train.shape[0] != predicted_y_train.shape[0] or true_y_test.shape[0] != predicted_y_test.shape[0]:
        print("Shape errors")
        return -1
    if adversary == 'regular':
        # All existing true labels
        y_list = np.unique(np.union1d(true_y_train, true_y_test))

        # All existing confidence scores
        y_pred_list = np.unique(np.union1d(predicted_y_train, predicted_y_test))
        
        # Train distrib
        train_distrib = {}
        for y in y_list: # For all true labels
            preds_for_this_y = [] # List of prediction scores for instances labeled with this label
            for i in range(true_y_train.shape[0]): # Loop over train instances
                if true_y_train[i] == y: # If instance has true label y
                    preds_for_this_y.append(predicted_y_train[i][1]) # Score associated to prediction 1 => ~score overall
            train_distrib[y] = np.unique(preds_for_this_y, return_counts=True) # Count occurences of each prediction score for this ground truth
        ok_cnt = 0 # ok_cnt and tot_cnt will be used to compute the model's accuracy
        tot_cnt = 0
        for y in y_list: # For each ground truth
            res = train_distrib[y] # Retrieve list of prediction scores associated to instances with this label (along with their counts)
            for i in range(res[0].size): # For each different prediction score
                tot_cnt += res[1][i] # Add the number of instances with this prediction score to global count
                if abs(res[0][i] - y) < 0.5: # If score indicates a correct prediction
                    ok_cnt += res[1][i] # Add the number of instances with this prediction score to correct predictions count
        if print_infos:
            print("Train Accuracy is ", ok_cnt/tot_cnt) 
        # Idem, test
        test_distrib = {}
        for y in y_list: # for each y
            preds_for_this_y = []
            for i in range(true_y_test.shape[0]):
                if true_y_test[i] == y: 
                    preds_for_this_y.append(predicted_y_test[i][1]) # < 0.5 if bad prediction, > 0.5 otherwise
            test_distrib[y] = np.unique(preds_for_this_y, return_counts=True)
        #print("Test Output Distribution")
        # Compute accuracy
        ok_cnt = 0
        tot_cnt = 0
        for y in y_list:
            res = test_distrib[y]
            for i in range(res[0].size):
                tot_cnt += res[1][i]
                if abs(res[0][i] - y) < 0.5:
                    ok_cnt += res[1][i]
                #print(res[0][i], ",", res[1][i])
        if print_infos:
            print("Test Accuracy is ", ok_cnt/tot_cnt)
        #print(test_distrib)

        # Now, compute vulnerability
        for y in y_list: # for each y
            # compute proba_y
            tot_y_train=0
            tot_y_test=0
            nb_y_train = np.sum(true_y_train == y)
            nb_y_test = np.sum(true_y_test == y)
            proba_y = (nb_y_train + nb_y_test)/(true_y_train.shape[0]+true_y_test.shape[0])
            #print("proba[y==",y,"]=",proba_y)
            # compute tau_y (distributional-overfitting distance)
            tau_y = 0
            for y_pred in y_pred_list: # for each \hat{y}
                index = np.where(train_distrib[y][0] == y_pred) # train instances with true label y and prediction \hat{y}
                #print(index[0])
                if len(index[0]) == 0:
                    proba_y_pred_train = 0
                else:
                    proba_y_pred_train = train_distrib[y][1][index][0]/nb_y_train # proportion of train instances with true label y and prediction \hat{y}
                #print("train", y_pred, ",", proba_y_pred_train)
                print("P(y_pred=%f | y=%d, M=1)=%f"%(y_pred, y, proba_y_pred_train))
                # idem, test
                index = np.where(test_distrib[y][0] == y_pred)
                if len(index[0]) == 0:
                    proba_y_pred_test = 0
                else:
                    proba_y_pred_test = test_distrib[y][1][index][0]/nb_y_test
                #print("test", y_pred, ",", proba_y_pred_test)
                print("P(y_pred=%f | y=%d, M=0)=%f"%(y_pred, y, proba_y_pred_test))
                tot_y_train+=proba_y_pred_train
                tot_y_test+=proba_y_pred_test
                tau_y += abs(proba_y_pred_train-proba_y_pred_test) # sum probas difference
            tau_y = (1/2)*tau_y # factor 1/2
            if print_infos:
                print("Distributional-Overfitting distance for class ", y, " is ", tau_y)
            if abs(tot_y_train - 1.0) > 0.0000001 or abs(tot_y_test - 1.0) > 0.0000001: # normalement c'est ok si petite erreur d'approx.
                print("error in proportions total count, exiting.")
                print("tot_y_train=", tot_y_train)
                print("tot_y_test=", tot_y_test)
                exit()
            advantage += proba_y*tau_y

        # plot tentative ------------------
        plot = False
        if plot:
            for y in y_list: # for each y
                probas_y_train = []
                probas_y_test = []
                for y_pred in y_pred_list: # for each \hat{y}
                    index = np.where(train_distrib[y][0] == y_pred)
                    #print(index[0])
                    if len(index[0]) == 0:
                        proba_y_pred_train = 0
                    else:
                        proba_y_pred_train = train_distrib[y][1][index][0]/nb_y_train
                    probas_y_train.append(proba_y_pred_train)
                    index = np.where(test_distrib[y][0] == y_pred)
                    if len(index[0]) == 0:
                        proba_y_pred_test = 0
                    else:
                        proba_y_pred_test = test_distrib[y][1][index][0]/nb_y_test
                    probas_y_test.append(proba_y_pred_test)
                from matplotlib import pyplot as plt
                plt.plot(y_pred_list, probas_y_train, label="Train")
                plt.plot(y_pred_list, probas_y_test, label="Test")
                plt.title('y=%d' %y)
                plt.legend()
                plt.show()
        # -----------------------------------
    elif adversary == 'discriminating':
        # All existing true labels
        y_list = np.unique(np.union1d(true_y_train, true_y_test))

        # All existing confidence scores
        y_pred_list = np.unique(np.union1d(predicted_y_train, predicted_y_test))
        
        # Train distrib
        train_distrib = {}
        for y in y_list: # For all true labels
            train_distrib[y] = {}
            # sensitive group
            preds_for_this_y = [] # List of prediction scores for instances labeled with this label and belonging to sensitive group
            for i in range(true_y_train.shape[0]): # Loop over train instances
                if true_y_train[i] == y and sensVect_train[i] == 1: # If instance has true label y and belongs to sensitive group
                    preds_for_this_y.append(predicted_y_train[i][1]) # Score associated to prediction 1 => ~score overall
            train_distrib[y]['protected'] = np.unique(preds_for_this_y, return_counts=True) # Count occurences of each prediction score for this ground truth
            # unsensitive group
            preds_for_this_y = [] # List of prediction scores for instances labeled with this label and belonging to sensitive group
            for i in range(true_y_train.shape[0]): # Loop over train instances
                if true_y_train[i] == y and unSensVect_train[i] == 1: # If instance has true label y and belongs to sensitive group
                    preds_for_this_y.append(predicted_y_train[i][1]) # Score associated to prediction 1 => ~score overall
            train_distrib[y]['unprotected'] = np.unique(preds_for_this_y, return_counts=True) # Count occurences of each prediction score for this ground truth
        ok_cnt = 0 # ok_cnt and tot_cnt will be used to compute the model's accuracy
        tot_cnt = 0
        for y in y_list: # For each ground truth
            res = train_distrib[y]['protected'] # Retrieve list of prediction scores associated to instances with this label (along with their counts)
            for i in range(res[0].size): # For each different prediction score
                tot_cnt += res[1][i] # Add the number of instances with this prediction score to global count
                if abs(res[0][i] - y) < 0.5: # If score indicates a correct prediction
                    ok_cnt += res[1][i] # Add the number of instances with this prediction score to correct predictions count
            res = train_distrib[y]['unprotected'] # Retrieve list of prediction scores associated to instances with this label (along with their counts)
            for i in range(res[0].size): # For each different prediction score
                tot_cnt += res[1][i] # Add the number of instances with this prediction score to global count
                if abs(res[0][i] - y) < 0.5: # If score indicates a correct prediction
                    ok_cnt += res[1][i] # Add the number of instances with this prediction score to correct predictions count
        if print_infos:
            print("Train Accuracy is ", ok_cnt/tot_cnt) 
        #print(train_distrib)
        # Idem, test
        test_distrib = {}
        for y in y_list: # For all true labels
            test_distrib[y] = {}
            # sensitive group
            preds_for_this_y = [] # List of prediction scores for instances labeled with this label and belonging to sensitive group
            for i in range(true_y_test.shape[0]): # Loop over train instances
                if true_y_test[i] == y and sensVect_test[i] == 1: # If instance has true label y and belongs to sensitive group
                    preds_for_this_y.append(predicted_y_test[i][1]) # Score associated to prediction 1 => ~score overall
            test_distrib[y]['protected'] = np.unique(preds_for_this_y, return_counts=True) # Count occurences of each prediction score for this ground truth
            # unsensitive group
            preds_for_this_y = [] # List of prediction scores for instances labeled with this label and belonging to sensitive group
            for i in range(true_y_test.shape[0]): # Loop over train instances
                if true_y_test[i] == y and unSensVect_test[i] == 1: # If instance has true label y and belongs to sensitive group
                    preds_for_this_y.append(predicted_y_test[i][1]) # Score associated to prediction 1 => ~score overall
            test_distrib[y]['unprotected'] = np.unique(preds_for_this_y, return_counts=True) # Count occurences of each prediction score for this ground truth
        ok_cnt = 0 # ok_cnt and tot_cnt will be used to compute the model's accuracy
        tot_cnt = 0
        for y in y_list: # For each ground truth
            res = test_distrib[y]['protected'] # Retrieve list of prediction scores associated to instances with this label (along with their counts)
            for i in range(res[0].size): # For each different prediction score
                tot_cnt += res[1][i] # Add the number of instances with this prediction score to global count
                if abs(res[0][i] - y) < 0.5: # If score indicates a correct prediction
                    ok_cnt += res[1][i] # Add the number of instances with this prediction score to correct predictions count
            res = test_distrib[y]['unprotected'] # Retrieve list of prediction scores associated to instances with this label (along with their counts)
            for i in range(res[0].size): # For each different prediction score
                tot_cnt += res[1][i] # Add the number of instances with this prediction score to global count
                if abs(res[0][i] - y) < 0.5: # If score indicates a correct prediction
                    ok_cnt += res[1][i] # Add the number of instances with this prediction score to correct predictions count
        if print_infos:
            print("Test Accuracy is ", ok_cnt/tot_cnt) 
        #print(test_distrib)
        #print(train_distrib)
        #print(test_distrib)
        # Now, compute vulnerability
        for y in y_list: # for each y
            for z in ['protected', 'unprotected']:
                # compute proba(y)
                tot_y_train=0
                tot_y_test=0
                nb_y_train = np.sum(true_y_train == y)
                nb_y_test = np.sum(true_y_test == y)
                if z == 'protected':
                    nb_z_train = np.sum(sens_vect_train == 1)
                    nb_z_test = np.sum(sens_vect_test == 1)
                else:
                    nb_z_train = np.sum(unsens_vect_train == 1)
                    nb_z_test = np.sum(unsens_vect_test == 1)
                proba_z = (nb_z_train + nb_z_test)/(true_y_train.shape[0]+true_y_test.shape[0])
                proba_y = (nb_y_train + nb_y_test)/(true_y_train.shape[0]+true_y_test.shape[0])
                if print_infos:
                    print("proba[z==",z,"]=",proba_z)
                    print("proba[y==",y,"]=",proba_y)
                # compute rho_z(y)
                if z == 'protected':
                    nb_y_z_train = len(np.intersect1d(np.where(true_y_train == y), np.where(sens_vect_train == 1)))
                    nb_y_z_test = len(np.intersect1d(np.where(true_y_test == y), np.where(sens_vect_test == 1)))
                    rho_z_y = (nb_y_z_train+nb_y_z_test)/(nb_z_train+nb_z_test)
                else:
                    nb_y_z_train = len(np.intersect1d(np.where(true_y_train == y), np.where(unsens_vect_train == 1)))
                    nb_y_z_test = len(np.intersect1d(np.where(true_y_test == y), np.where(unsens_vect_test == 1)))
                    rho_z_y = (nb_y_z_train+nb_y_z_test)/(nb_z_train+nb_z_test)
                if print_infos:
                    print("rho_z_y (z=%s, y=%d)=%f" %(z,y,rho_z_y))
                # compute tau_z(y) (subgroup distributional-overfitting distance)
                tau_z_y = 0
                for y_pred in y_pred_list: # for each \hat{y}
                    index = np.where(train_distrib[y][z][0] == y_pred) # train instances with true label y and prediction \hat{y}
                    #print(index[0])
                    if len(index[0]) == 0:
                        proba_y_pred_train = 0
                    else:
                        proba_y_pred_train = train_distrib[y][z][1][index][0]/nb_y_z_train # proportion of train instances with true label y and prediction \hat{y}
                    #print("train", y_pred, ",", proba_y_pred_train)
                    print("P(y_pred=%f | y=%d, z=%s, M=1)=%f"%(y_pred, y, z, proba_y_pred_train))
                    # idem, test
                    index = np.where(test_distrib[y][z][0] == y_pred)
                    if len(index[0]) == 0:
                        proba_y_pred_test = 0
                    else:
                        proba_y_pred_test = test_distrib[y][z][1][index][0]/nb_y_z_test
                    #print("test", y_pred, ",", proba_y_pred_test)
                    print("P(y_pred=%f | y=%d, z=%s, M=0)=%f"%(y_pred, y, z, proba_y_pred_test))
                    tot_y_train+=proba_y_pred_train
                    tot_y_test+=proba_y_pred_test
                    tau_z_y += abs(proba_y_pred_train-proba_y_pred_test) # sum probas difference
                tau_z_y = (1/2)*tau_z_y # factor 1/2
                if print_infos:
                    print("Distributional-Overfitting distance for class ", y, "and class %s" %z," is ", tau_z_y)
                if abs(tot_y_train - 1.0) > 0.0000001 or abs(tot_y_test - 1.0) > 0.0000001: # normalement c'est ok si petite erreur d'approx.
                    print("error in proportions total count, exiting.")
                    print("tot_y_train=", tot_y_train)
                    print("tot_y_test=", tot_y_test)
                    exit()
                advantage += proba_z*rho_z_y*tau_z_y
    else:
        print("Unknown mode ", adversary)
    return 0.5 + 0.5*advantage # overall vulnerability value for regular adversary

def filter_subgroup(vect, filt_vect):
    #print("Shape before : ", vect.shape)
    ret_vect = vect[np.where(filt_vect==1)]
    #print("Shape after : ", ret_vect.shape)
    return ret_vect

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)
print("Using ", X_train.shape[0], " examples for training set, ", X_test.shape[0], " examples for test set.")

# Separate protected features to avoid disparate treatment

# Training set
sensVect_train =  X_train[:,sensitive_attr_column]
unSensVect_train =  X_train[:,unsensitive_attr_column] 
X_train_unprotected = X_train[:,2:]

# Test set
sensVect_test =  X_test[:,sensitive_attr_column]
unSensVect_test =  X_test[:,unsensitive_attr_column] 
X_test_unprotected = X_test[:,2:]


# Train the target model.
debug=False

print("Training the target model...")
target_model = target_model_fn()
if debug:
    # Train it
    # To avoid re-training:
    import pickle
    try:
        with open('model_saved_%s_%s' %(predict_proba_mode, dataset), 'rb') as handle:
            target_model = pickle.load(handle)
            print("--- Found local save, no need to re-train the model")
    except:
        print("--- Did not find local save, training model")
        target_model.fit(X_train_unprotected, y_train, features=features[2:])
        with open('model_saved_%s_%s' %(predict_proba_mode, dataset), 'wb') as handle:
            pickle.dump(target_model, handle, protocol=pickle.HIGHEST_PROTOCOL)
            
else:
    target_model.fit(X_train_unprotected, y_train, features=features[2:])

print("Trained RL (target model):", target_model.rl_.get_long_repr())
perfs = dict()
perfs['train_acc']=target_model.score(X_train_unprotected, y_train)
perfs['train_unf']=compute_unfairness(sensVect_train, unSensVect_train, y_train, target_model.predict(X_train_unprotected))
perfs['test_acc']=target_model.score(X_test_unprotected, y_test)
perfs['test_unf']=compute_unfairness(sensVect_test, unSensVect_test, y_test, target_model.predict(X_test_unprotected))
perfs['length']=target_model.rl_.get_length()

#print('train preds are:', np.unique(y_train_pred))
#print('test preds are:', np.unique(y_test_pred))
adversaries = ['regular', 'discriminating']
attack_perfs = dict()
#predict_modes = ['label_only']
for adversary in adversaries:
    attack_data = {}
    for predict_mode in predict_modes:
        target_model.predict_proba_mode = predict_mode
        y_train_pred = target_model.predict_proba(X_train_unprotected)
        y_test_pred = target_model.predict_proba(X_test_unprotected)
        print("----- Mode %s (%s attacker)------" %(predict_mode, adversary))
        vuln = compute_vulnerability_simpler(adversary, y_train, y_train_pred, y_test, y_test_pred, sensVect_train, sensVect_test, unSensVect_train, unSensVect_test, print_infos=True)
        print("Overall vulnerability is ", vuln)
        """print("-- Sensitive group (train proportion %f, test proportion %f)" %(np.sum(sensVect_train == 1)/y_train.shape[0], np.sum(sensVect_test == 1)/y_test.shape[0]))
        vuln_sens = compute_vulnerability(adversary, filter_subgroup(y_train, sensVect_train), filter_subgroup(y_train_pred, sensVect_train), filter_subgroup(y_test, sensVect_test), filter_subgroup(y_test_pred, sensVect_test))
        print(" Vulnerability is " , vuln_sens)
        print("-- Unsensitive group (train proportion %f, test proportion %f) " %(np.sum(unSensVect_train == 1)/y_train.shape[0], np.sum(unSensVect_test == 1)/y_test.shape[0]))
        vuln_unsens = compute_vulnerability(adversary, filter_subgroup(y_train, unSensVect_train), filter_subgroup(y_train_pred, unSensVect_train), filter_subgroup(y_test, unSensVect_test), filter_subgroup(y_test_pred, unSensVect_test))
        print(" Vulnerability is ",  vuln_unsens)"""
        attack_data[predict_mode] = vuln
        print("--------------------")
    attack_perfs[adversary] = attack_data

fileName = './results/vulnerability_report_%s_%s_%s_%f_%d_%d_%d_%f.csv' %(method, dataset, policy, epsilon, fairnessMetric, time_limit, seed, lambdaParam)
with open(fileName, mode='w') as csv_file:
    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    csv_writer.writerow(['Seed', 'Training accuracy', 'Training Unfairness(%d)' %fairnessMetric, 'Test accuracy', 'Test unfairness', 'Length'])#, 'Fairness STD', 'Accuracy STD'])
    csv_writer.writerow([seed, perfs['train_acc'], perfs['train_unf'], perfs['test_acc'], perfs['test_unf'], perfs['length']])#, 'Fairness STD', 'Accuracy STD'])
    csv_writer.writerow(['Adversary', 'Label-only vulnerability', 'Score vulnerability', 'Rule ID vulnerability'])#, 'Fairness STD', 'Accuracy STD'])
    for adv in adversaries:
        csv_writer.writerow([adv, attack_perfs[adv][predict_modes[0]], attack_perfs[adv][predict_modes[1]], attack_perfs[adv][predict_modes[2]]])

